<!DOCTYPE html>
<html>

<head lang="en">
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Ponder</title>

    <!-- <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta property="og:image" content="https://dorverbin.github.io/refnerf/img/refnerf_titlecard.jpg">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="1200">
    <meta property="og:image:height" content="630">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://dorverbin.github.io/refnerf">
    <meta property="og:title" content="Ref-NeRF: Structured View-Dependent Appearance for Neural Radiance Fields">
    <meta property="og:description"
        content="Neural Radiance Fields (NeRF) is a popular view synthesis technique that represents a scene as a continuous volumetric function, parameterized by multilayer perceptrons that provide the volume density and view-dependent emitted radiance at each location. While NeRF-based techniques excel at representing fine geometric structures with smoothly varying view-dependent appearance, they often fail to accurately capture and reproduce the appearance of glossy surfaces. We address this limitation by introducing Ref-NeRF, which replaces NeRF's parameterization of view-dependent outgoing radiance with a representation of reflected radiance and structures this function using a collection of spatially-varying scene properties. We show that together with a regularizer on normal vectors, our model significantly improves the realism and accuracy of specular reflections. Furthermore, we show that our model's internal representation of outgoing radiance is interpretable and useful for scene editing.">

    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Ref-NeRF: Structured View-Dependent Appearance for Neural Radiance Fields">
    <meta name="twitter:description"
        content="Neural Radiance Fields (NeRF) is a popular view synthesis technique that represents a scene as a continuous volumetric function, parameterized by multilayer perceptrons that provide the volume density and view-dependent emitted radiance at each location. While NeRF-based techniques excel at representing fine geometric structures with smoothly varying view-dependent appearance, they often fail to accurately capture and reproduce the appearance of glossy surfaces. We address this limitation by introducing Ref-NeRF, which replaces NeRF's parameterization of view-dependent outgoing radiance with a representation of reflected radiance and structures this function using a collection of spatially-varying scene properties. We show that together with a regularizer on normal vectors, our model significantly improves the realism and accuracy of specular reflections. Furthermore, we show that our model's internal representation of outgoing radiance is interpretable and useful for scene editing.">
    <meta name="twitter:image" content="https://dorverbin.github.io/refnerf/img/refnerf_titlecard.jpg"> -->


    <!-- mirror: F0%9F%AA%9E&lt -->
    <!-- <link rel="icon"
        href="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text y=%22.9em%22 font-size=%2290%22&gt;%E2%9C%A8&lt;/text&gt;&lt;/svg&gt;"> -->
    <link rel="stylesheet" href="css/bootstrap.min.css">
    <link rel="stylesheet" href="css/font-awesome.min.css">
    <link rel="stylesheet" href="css/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <script src="js/jquery.min.js"></script>
    <script src="js/bootstrap.min.js"></script>
    <script src="js/codemirror.min.js"></script>
    <script src="js/clipboard.min.js"></script>
    <script src="js/video_comparison.js"></script>
    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="header" style="text-align: center; margin: auto;">
        <div class="row" id="title-row" style="max-width: 100%; margin: 0 auto; display: inline-block">
            <h2 class="col-md-12 text-center" id="title">
                <b>Ponder</b>: Point Cloud Pre-training via <br> Neural Rendering <br>
                <small>
                    ICCV 2023
                </small>
            </h2>
        </div>
        <div class="row" id="author-row" style="margin:0 auto;">
            <div class="col-md-12 text-center" style="display: table; margin:0 auto">
                <table class="author-table" id="author-table">
                    <tr>
                        <td>
                            <a style="text-decoration:none" href="https://dihuang.me/">
                                Di Huang
                            </a>
                            <br>The University of Sydney<br>Shanghai AI Laboratory
                        </td>
                        <td>
                            <a style="text-decoration:none" href="https://pengsida.net/">
                                Sida Peng
                            </a>
                            <br>Zhejiang University
                        </td>
                        <td>
                            <a style="text-decoration:none" href="https://tonghe90.github.io/">
                                Tong He*
                            </a>
                            <br>Shanghai AI Laboratory
                        </td>
                    </tr>
                    <tr>
                        <td>
                            <a style="text-decoration:none"
                                href="https://scholar.google.com/citations?user=ee8k3qcAAAAJ&hl=en">
                                Honghui Yang
                            </a>
                            <br>Zhejiang University
                        </td>
                        <td>
                            <a style="text-decoration:none" href="https://www.xzhou.me/">
                                Xiaowei Zhou
                            </a>
                            <br>Zhejiang University
                        </td>
                        <td>
                            <a style="text-decoration:none" href="https://wlouyang.github.io/">
                                Wanli Ouyang
                            </a>
                            <br>Shanghai AI Laboratory
                        </td>
                    </tr>
                </table>
            </div>
        </div>
    </div>
    <script>
        document.getElementById('author-row').style.maxWidth = document.getElementById("title-row").clientWidth + 'px';
    </script>
    <div class="container" id="main">
        <div class="row">
            <div class="col-sm-6 col-sm-offset-3 text-center">
                <ul class="nav nav-pills nav-justified">
                    <li>
                        <a href="https://arxiv.org/abs/2301.00157">
                            <img src="img/Snipaste_2023-10-26_20-22-55.png" height="60px">
                            <h4><strong>Ponder Paper</strong></h4>
                        </a>
                    </li>
                    <li>
                        <a href="https://arxiv.org/abs/2310.08586">
                            <img src="img/Snipaste_2023-10-26_20-24-39.png" height="60px">
                            <h4><strong>PonderV2 Paper</strong></h4>
                        </a>
                    </li>
                    <!-- <li>
                            <a href="https://youtu.be/qrdRH9irAlk">
                            <img src="./img/youtube_icon.png" height="60px">
                                <h4><strong>Video</strong></h4>
                            </a>
                        </li> -->
                    <!-- <li>
                            <a href="https://storage.googleapis.com/gresearch/refraw360/ref.zip" target="_blank">
                            <image src="img/database_icon.png" height="60px">
                                <h4><strong>Shiny Dataset</strong></h4>
                            </a>
                        </li> -->
                    <!-- <li>  
                            <a href="https://dorverbin.github.io/refnerf/data/shiny_blender_source.zip" target="_blank">
                            <image src="img/database_icon.png" height="60px">
                                <h4><strong>Shiny Dataset Source</strong></h4>
                            </a>
                        </li> -->
                    <!-- <li>
                            <a href="https://storage.googleapis.com/gresearch/refraw360/ref_real.zip" target="_blank">
                            <image src="img/real_database_icon.png" height="60px">
                                <h4><strong>Real Dataset</strong></h4>
                            </a>
                        </li>                             -->
                    <li>
                        <a href="https://github.com/OpenGVLab/PonderV2" target="_blank">
                            <image src="img/github.png" height="60px">
                                <h4><strong>Code</strong></h4>
                        </a>
                    </li>
                </ul>
            </div>
        </div>

        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <div class="video-compare-container" id="materialsDiv">
                    <video class="video" id="materials" loop playsinline autoPlay muted src="video/materials_circle_mipnerf_ours.mp4" onplay="resizeAndPlay(this)"></video>
                    
                    <canvas height=0 class="videoMerge" id="materialsMerge"></canvas>
                </div>
			</div>
        </div> -->


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract & Pipeline
                </h3>
                <p class="text-justify">
                    We propose a novel approach to self-supervised learning of point cloud representations by
                    differentiable neural rendering. Motivated by the fact that informative point cloud features should
                    be able to encode rich geometry and appearance cues and render realistic images, we train a
                    point-cloud encoder within a devised point-based neural renderer by comparing the rendered images
                    with real images on massive RGB-D data. The learned point-cloud encoder can be easily integrated
                    into various downstream tasks, including not only high-level tasks like 3D detection and
                    segmentation, but low-level tasks like 3D reconstruction and image synthesis. Extensive experiments
                    on various tasks demonstrate the superiority of our approach compared to existing pre-training
                    methods.
                </p>
            </div>
        </div>

        <image src="img/pipeline.png" class="img-responsive" alt="overview" width="60%"
            style="max-height: 450px;margin:auto;">
            <br>

            <div class="row">
                <div class="col-md-8 col-md-offset-2">
                    <p class="text-justify">
                        The pipeline of our point cloud pre-training via neural rendering (Ponder). Given multi-view
                        RGB-D images, we first
                        construct the point cloud by back-projection, then use a point cloud encoder to extract
                        per-point features. Those features are organized to a 3D
                        feature volume (visualized as an image in this figure) by average pooling. Finally, the 3D
                        feature volume is rendered to multi-view RGB-D
                        images via a differentiable neural rendering, which are compared with the original input
                        multi-view RGB-D images as the supervision.
                        Point cloud encoder and color decoder are used for transfer learning on downstream tasks.
                    </p>
                </div>
            </div>

            <div class="row">
                <div class="col-md-8 col-md-offset-2">
                    <h3>
                        Performance
                    </h3>
                    <div class="text-justify">
                        Ponder achieves state-of-the-art performance on various downstream tasks, e.g., 3D semantic
                        segmentation on ScanNet and ScanNet-200.
                    </div>
                    <br>
                    <div class="text-center">
                        <img src="img/eeeee10492aa63c5e9318476caf13c95.PNG" width="100%">
                    </div>
                    <div class="text-center">
                        <img src="img/2beb9025cd119848353c6a2c6a3a6015.PNG" width="100%">
                    </div>
                    <br>
                    <!-- <div class="text-justify">
                        We call this <i>Integrated Directional Encoding</i>, and we show experimentally that it allows
                        sharing the emittance functions between points with different roughnesses. It also enables scene
                        editing after training. Theoretically, our encoding is stationary on the sphere, similar to the
                        Euclidean stationarity of NeRF's positional encoding. <br><br>
                    </div> -->
                </div>
            </div>

            <!-- <div class="row">
                <div class="col-md-8 col-md-offset-2">
                    <h3>
                        Video
                    </h3>
                    <div class="text-center">
                        <div style="position:relative;padding-top:56.25%;">
                            <iframe src="https://youtube.com/embed/qrdRH9irAlk" allowfullscreen
                                style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                        </div>
                    </div>
                </div>
            </div> -->

            <!-- <div class="row">
                <div class="col-md-8 col-md-offset-2">
                    <h3>
                        Reflection Direction Parameterization
                    </h3>
                    <div class="text-justify">
                        Previous approaches directly input the camera's view direction into the MLP to predict outgoing
                        radiance. We show that instead using the reflection of the view direction about the normal makes
                        the emittance function significantly easier to learn and interpolate, greatly improving our
                        results.

                        <br><br>

                    </div>
                    <div class="text-center">
                        <video id="refdir" width="40%" playsinline autoplay loop muted>
                            <source src="video/reflection_animation.mp4" type="video/mp4" />
                        </video>
                    </div>
                </div>
            </div> -->

            <!-- <div class="row">
                <div class="col-md-8 col-md-offset-2">
                    <h3>
                        Integrated Directional Encoding
                    </h3>
                    <div class="text-justify">
                        We explicitly model object roughness using the expected values of a set of spherical harmonics
                        under a von Mises-Fisher distribution whose concentration parameter varies spatially:
                    </div>
                    <br>
                    <div class="text-center">
                        <img src="./img/ide.png" width="50%">
                    </div>
                    <br>
                    <div class="text-justify">
                        We call this <i>Integrated Directional Encoding</i>, and we show experimentally that it allows
                        sharing the emittance functions between points with different roughnesses. It also enables scene
                        editing after training. Theoretically, our encoding is stationary on the sphere, similar to the
                        Euclidean stationarity of NeRF's positional encoding. <br><br>
                    </div>
                    <div class="text-center">
                        <video id="ide" width="100%" playsinline autoplay controls loop muted>
                            <source src="video/ide_animation.mp4" type="video/mp4" />
                        </video>
                    </div>
                </div>
            </div> -->

            <!-- <div class="row">
                <div class="col-md-8 col-md-offset-2">
                    <h3>
                        Additional Synthetic Results
                    </h3>
                    <div class="video-compare-container">
                        <video class="video" id="musclecar" loop playsinline autoPlay muted
                            src="video/musclecar_mipnerf_ours.mp4" onplay="resizeAndPlay(this)"></video>
                        <canvas height=0 class="videoMerge" id="musclecarMerge"></canvas>
                    </div>
                </div>
            </div> -->

            <!-- <div class="row">
                <div class="col-md-8 col-md-offset-2">
                    <h3>
                        Results on Captured Scenes
                    </h3>
                    Our method also produces accurate renderings and surface normals from captured photographs:
                    <div class="video-compare-container" style="width: 100%">
                        <video class="video" id="toycar" loop playsinline autoPlay muted src="video/toycar.mp4"
                            onplay="resizeAndPlay(this)"></video>
                        <canvas height=0 class="videoMerge" id="toycarMerge"></canvas>
                    </div>
                </div>
            </div> -->

            <!-- <div class="row">
                <div class="col-md-8 col-md-offset-2">
                    <h3>
                        Scene Editing
                    </h3>
                    <div class="text-justify">
                        We show that our structured representation of the directional MLP allows for scene editing after
                        training. Here we show that we can convincingly change material properties.
                        <br>
                        We can increase and decrease material roughness:
                    </div>

                    <div style="overflow: hidden;">
                        <video id="editing-materials" width="100%" playsinline autoplay loop muted
                            style="margin-top: -5%;">
                            <source src="video/materials_rougher_smoother.mp4" type="video/mp4" />
                        </video>
                    </div>

                    <div class="text-justify">
                        We can also control the amounts of specular and diffuse colors, or change the diffuse color
                        without affecting the specular reflections:
                    </div>

                    <table width="100%">
                        <tr>
                            <td align="left" valign="top" width="50%">
                                <video id="v2" width="100%" playsinline autoplay loop muted>
                                    <source src="video/car_color2.mp4" type="video/mp4" />
                                </video>
                            </td>
                            <td align="left" valign="top" width="50%">
                                <video id="v3" width="100%" playsinline autoplay loop muted>
                                    <source src="video/car_color3.mp4" type="video/mp4" />
                                </video>
                            </td>
                        </tr>
                    </table>

                </div>
            </div> -->


            <div class="row">
                <div class="col-md-8 col-md-offset-2">
                    <h3>
                        Citation
                    </h3>
                    <div class="form-group col-md-10 col-md-offset-1">
                        <textarea id="bibtex" class="form-control" readonly>
@inproceedings{huang2023ponder,
    title={Ponder: Point cloud pre-training via neural rendering},
    author={Huang, Di and Peng, Sida and He, Tong and Yang, Honghui and Zhou, Xiaowei and Ouyang, Wanli},
    booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
    pages={16089--16098},
    year={2023}
  }
</textarea>
                    </div>
                </div>
            </div>

            <div class="row">
                <div class="col-md-8 col-md-offset-2">
                    <!-- <h3>
                        Acknowledgements
                    </h3> -->
                    <p class="text-justify">
                        <!-- We would like to thank Lior Yariv and Kai Zhang for helping us evaluate their methods, and
                        Ricardo Martin-Brualla for helpful comments on our text. DV is supported by the National Science
                        Foundation under Cooperative Agreement PHY-2019786 (an NSF AI Institute, <a
                            href="http://iaifi.org">http://iaifi.org</a>)
                        <br> -->
                        The website template was borrowed from <a href="http://mgharbi.com/">Michaël Gharbi</a> and <a
                            href="https://dorverbin.github.io/refnerf/">Ref-NeRF</a>. Thanks to them!
                    </p>
                </div>
            </div>
    </div>


<!-- Default Statcounter code for Ponder
https://dihuangdh.github.io/ponder -->
<script type="text/javascript">
    var sc_project=12946926; 
    var sc_invisible=1; 
    var sc_security="d7ea4aa9"; 
    </script>
    <script type="text/javascript"
    src="https://www.statcounter.com/counter/counter.js"
    async></script>
    <noscript><div class="statcounter"><a title="Web Analytics"
    href="https://statcounter.com/" target="_blank"><img
    class="statcounter"
    src="https://c.statcounter.com/12946926/0/d7ea4aa9/1/"
    alt="Web Analytics"
    referrerPolicy="no-referrer-when-downgrade"></a></div></noscript>
    <!-- End of Statcounter Code -->

</body>

</html>